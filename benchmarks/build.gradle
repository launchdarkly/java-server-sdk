
buildscript {
    repositories {
        mavenCentral()
    }
}

plugins {
    id "me.champeau.gradle.jmh" version "0.5.0"
    id "io.morethan.jmhreport" version "0.9.0"
}

repositories {
    mavenCentral()
}

ext.versions = [
    "jmh": "1.21",
    "guava": "19.0"
]

dependencies {
    implementation files("lib/launchdarkly-java-server-sdk.jar")
    implementation files("lib/launchdarkly-java-server-sdk-test.jar")
    implementation "com.google.code.gson:gson:2.8.9"
    implementation "com.google.guava:guava:${versions.guava}" // required by SDK test code
    implementation "com.squareup.okhttp3:mockwebserver:3.12.10"
    implementation "org.openjdk.jmh:jmh-core:1.21"
    implementation "org.openjdk.jmh:jmh-generator-annprocess:${versions.jmh}"
}

// need to set duplicatesStrategy because otherwise some non-class files with
// duplicate names in our dependencies will cause an error
tasks.getByName('jmhJar').doFirst() {duplicatesStrategy(DuplicatesStrategy.EXCLUDE)}

jmh {
    iterations = 10 // Number of measurement iterations to do.
    benchmarkMode = ['avgt'] // "average time" - reports execution time as ns/op and allocations as B/op.
    // batchSize = 1 // Batch size: number of benchmark method calls per operation. (some benchmark modes can ignore this setting)
    fork = 1 // How many times to forks a single benchmark. Use 0 to disable forking altogether
    // failOnError = false // Should JMH fail immediately if any benchmark had experienced the unrecoverable error?
    forceGC = true // Should JMH force GC between iterations?
    humanOutputFile = project.file("${project.buildDir}/reports/jmh/human.txt") // human-readable output file
    // resultsFile = project.file("${project.buildDir}/reports/jmh/results.txt") // results file
    operationsPerInvocation = 3 // Operations per invocation.
    // benchmarkParameters =  [:] // Benchmark parameters.
    profilers = [ 'gc' ] // Use profilers to collect additional data. Supported profilers: [cl, comp, gc, stack, perf, perfnorm, perfasm, xperf, xperfasm, hs_cl, hs_comp, hs_gc, hs_rt, hs_thr]
    timeOnIteration = '1s' // Time to spend at each measurement iteration.
    resultFormat = 'JSON' // Result format type (one of CSV, JSON, NONE, SCSV, TEXT)
    // synchronizeIterations = false // Synchronize iterations?
    // threads = 4 // Number of worker threads to run with.
    // timeout = '1s' // Timeout for benchmark iteration.
    timeUnit = 'ns' // Output time unit. Available time units are: [m, s, ms, us, ns].
    verbosity = 'NORMAL' // Verbosity mode. Available modes are: [SILENT, NORMAL, EXTRA]
    warmup = '1s' // Time to spend at each warmup iteration.
    warmupBatchSize = 2 // Warmup batch size: number of benchmark method calls per operation.
    warmupIterations = 1 // Number of warmup iterations to do.
    // warmupForks = 0 // How many warmup forks to make for a single benchmark. 0 to disable warmup forks.
    // warmupMode = 'INDI' // Warmup mode for warming up selected benchmarks. Warmup modes are: [INDI, BULK, BULK_INDI].

    jmhVersion = versions.jmh
}

jmhReport {
    jmhResultPath = project.file('build/reports/jmh/results.json')
    jmhReportOutput = project.file('build/reports/jmh')
}
